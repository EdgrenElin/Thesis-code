{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import  cosine_similarity\n",
    "from inference_utils.pytorch_data_utils import __loadCLSembeddings__\n",
    "from inference_utils.pytorch_data_utils import __loadtrainingdf__\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import pearsonr, spearmanr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold1_seed41_training_results_final_epoch.pkl.zip',compression='zip')\n",
    "val1_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold1_seed41_validation_results_final_epoch.pkl.zip',compression='zip')\n",
    "train2_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold2_seed41_training_results_final_epoch.pkl.zip',compression='zip')\n",
    "val2_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold2_seed41_validation_results_final_epoch.pkl.zip',compression='zip')\n",
    "train3_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold3_seed41_training_results_final_epoch.pkl.zip',compression='zip')\n",
    "val3_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold3_seed41_validation_results_final_epoch.pkl.zip',compression='zip')\n",
    "train4_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold4_seed41_training_results_final_epoch.pkl.zip',compression='zip')\n",
    "val4_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold4_seed41_validation_results_final_epoch.pkl.zip',compression='zip')\n",
    "train5_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold5_seed41_training_results_final_epoch.pkl.zip',compression='zip')\n",
    "val5_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold5_seed41_validation_results_final_epoch.pkl.zip',compression='zip')\n",
    "train6_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold6_seed41_training_results_final_epoch.pkl.zip',compression='zip')\n",
    "val6_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold6_seed41_validation_results_final_epoch.pkl.zip',compression='zip')\n",
    "train7_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold7_seed41_training_results_final_epoch.pkl.zip',compression='zip')\n",
    "val7_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold7_seed41_validation_results_final_epoch.pkl.zip',compression='zip')\n",
    "train8_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold8_seed41_training_results_final_epoch.pkl.zip',compression='zip')\n",
    "val8_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold8_seed41_validation_results_final_epoch.pkl.zip',compression='zip')\n",
    "train9_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold9_seed41_training_results_final_epoch.pkl.zip',compression='zip')\n",
    "val9_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold9_seed41_validation_results_final_epoch.pkl.zip',compression='zip')\n",
    "train10_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold10_seed41_training_results_final_epoch.pkl.zip',compression='zip')\n",
    "val10_data = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/without_duration/fold10_seed41_validation_results_final_epoch.pkl.zip',compression='zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train1_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold1_seed41_training_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "val1_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold1_seed41_validation_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "train2_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold2_seed41_training_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "val2_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold2_seed41_validation_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "train3_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold3_seed41_training_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "val3_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold3_seed41_validation_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "train4_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold4_seed41_training_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "val4_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold4_seed41_validation_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "train5_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold5_seed41_training_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "val5_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold5_seed41_validation_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "train6_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold6_seed41_training_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "val6_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold6_seed41_validation_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "train7_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold7_seed41_training_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "val7_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold7_seed41_validation_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "train8_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold8_seed41_training_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "val8_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold8_seed41_validation_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "train9_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold9_seed41_training_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "val9_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold9_seed41_validation_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "train10_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold10_seed41_training_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n",
    "val10_emb = pd.read_pickle('../../../../storage/shared/master_thesis_TRIDENT/TRIDENT_for_Elin/TRIDENT_fish_EC50_10foldCV_35epochs_for_Elin/fold10_seed41_validation_SMILES_to_CLS_final_epoch.pkl.zip',compression='zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1=pd.merge(train1_data,train1_emb,on='SMILES_Canonical_RDKit')\n",
    "train2=pd.merge(train2_data,train2_emb,on='SMILES_Canonical_RDKit')\n",
    "train3=pd.merge(train3_data,train3_emb,on='SMILES_Canonical_RDKit')\n",
    "train4=pd.merge(train4_data,train4_emb,on='SMILES_Canonical_RDKit')\n",
    "train5=pd.merge(train5_data,train5_emb,on='SMILES_Canonical_RDKit')\n",
    "train6=pd.merge(train6_data,train6_emb,on='SMILES_Canonical_RDKit')\n",
    "train7=pd.merge(train7_data,train7_emb,on='SMILES_Canonical_RDKit')\n",
    "train8=pd.merge(train8_data,train8_emb,on='SMILES_Canonical_RDKit')\n",
    "train9=pd.merge(train9_data,train9_emb,on='SMILES_Canonical_RDKit')\n",
    "train10=pd.merge(train10_data,train10_emb,on='SMILES_Canonical_RDKit')\n",
    "\n",
    "val1=pd.merge(val1_data,val1_emb,on='SMILES_Canonical_RDKit')\n",
    "val2=pd.merge(val2_data,val2_emb,on='SMILES_Canonical_RDKit')\n",
    "val3=pd.merge(val3_data,val3_emb,on='SMILES_Canonical_RDKit')\n",
    "val4=pd.merge(val4_data,val4_emb,on='SMILES_Canonical_RDKit')\n",
    "val5=pd.merge(val5_data,val5_emb,on='SMILES_Canonical_RDKit')\n",
    "val6=pd.merge(val6_data,val6_emb,on='SMILES_Canonical_RDKit')\n",
    "val7=pd.merge(val7_data,val7_emb,on='SMILES_Canonical_RDKit')\n",
    "val8=pd.merge(val8_data,val8_emb,on='SMILES_Canonical_RDKit')\n",
    "val9=pd.merge(val9_data,val9_emb,on='SMILES_Canonical_RDKit')\n",
    "val10=pd.merge(val10_data,val10_emb,on='SMILES_Canonical_RDKit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Put together for easier handling\n",
    "dataframes = {\n",
    "    \"train\": [train1, train2, train3, train4, train5, train6, train7, train8, train9, train10],\n",
    "    \"val\": [val1, val2, val3, val4, val5, val6, val7, val8, val9, val10]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get residuals and L1 \n",
    "residuals = {\"train\": [], \"val\": []}\n",
    "L1 = {\"train\": [], \"val\": []}\n",
    "\n",
    "for key, df_list in dataframes.items():\n",
    "    for df in df_list:\n",
    "        df_residuals = df['labels'] - df['preds']\n",
    "        \n",
    "        L1_fold = np.abs(df_residuals)\n",
    "        \n",
    "        residuals[key].append(df_residuals)\n",
    "        L1[key].append(L1_fold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    dataframes['train'][i]['pred_L1_error']=L1['train'][i]\n",
    "    dataframes['val'][i]['pred_L1_error']=L1['val'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    embeddings_train_fold=np.asarray(dataframes['train'][i].CLS_embeddings.tolist(), dtype=np.float32)\n",
    "    embeddings_val_fold=np.asarray(dataframes['val'][i].CLS_embeddings.tolist(), dtype=np.float32)\n",
    "    dataframes['val'][i]['distance_to_training'] = np.nan\n",
    "\n",
    "    for j in range(len(embeddings_val_fold)):\n",
    "        embeddings_val_fold_current=embeddings_val_fold[j].reshape(1, -1)\n",
    "        similarity=cosine_similarity(embeddings_train_fold,embeddings_val_fold_current).mean()\n",
    "        dataframes['val'][i].at[j, 'distance_to_training'] = similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add validation fold data toghter for the new model CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb=[dataframes['val'][0],dataframes['val'][1],dataframes['val'][2],dataframes['val'][3],dataframes['val'][4],dataframes['val'][5],dataframes['val'][6],dataframes['val'][7],dataframes['val'][8],dataframes['val'][9]]\n",
    "combined_df = pd.concat(comb, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get residuals and L1 \n",
    "residuals = {\"train\": [], \"val\": []}\n",
    "L1 = {\"train\": [], \"val\": []}\n",
    "\n",
    "for key, df_list in dataframes.items():\n",
    "    for df in df_list:\n",
    "        df_residuals = df['labels'] - df['preds']\n",
    "        \n",
    "        L1_fold = np.abs(df_residuals)\n",
    "        \n",
    "        residuals[key].append(df_residuals)\n",
    "        L1[key].append(L1_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_svr_with_cv(X_train, y_train, n_splits=10):\n",
    "    # Create split for data into folds\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Define the hyperparameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'kernel': ['poly', 'rbf'],\n",
    "        'C': [0.05,0.1, 0.2],\n",
    "        'epsilon': [ 0.2,0.3,0.4,0.5]\n",
    "    }\n",
    "    \n",
    "    # Define the SVR model\n",
    "    svr = SVR()\n",
    "\n",
    "    # Use GridSearchCV to find the best parameters\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=svr,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',  # Optimize for negative MSE\n",
    "        cv=kf,  # Use K-Fold cross-validation\n",
    "        verbose=1,  # Show progress\n",
    "        n_jobs=-1  \n",
    "    )\n",
    "    \n",
    "    # Fit GridSearchCV\n",
    "    print(\"Performing GridSearchCV...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Extract the best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Collect correlation metrics across folds\n",
    "    train_correlations_pearson = []\n",
    "    val_correlations_pearson = []\n",
    "    train_correlations_spearman = []\n",
    "    val_correlation_spearman = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_train):  # Split data into train and val folds\n",
    "        # Split data into training and validation folds\n",
    "        embeddings_train_fold, embeddings_val_fold = X_train[train_idx], X_train[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        # Train the best model on the fold\n",
    "        best_model.fit(embeddings_train_fold, y_train_fold)\n",
    "        y_val_pred = best_model.predict(embeddings_val_fold)\n",
    "        y_train_pred = best_model.predict(embeddings_train_fold)\n",
    "\n",
    "        # Compute Pearson and Spearman correlations\n",
    "        train_corr_pearson = pearsonr(y_train_fold, y_train_pred)[0]\n",
    "        val_corr_pearson = pearsonr(y_val_fold, y_val_pred)[0]\n",
    "        train_corr_spearman = spearmanr(y_train_fold, y_train_pred)[0]\n",
    "        val_corr_spearman = spearmanr(y_val_fold, y_val_pred)[0]\n",
    "\n",
    "        train_correlations_pearson.append(train_corr_pearson)\n",
    "        val_correlations_pearson.append(val_corr_pearson)\n",
    "        train_correlations_spearman.append(train_corr_spearman)\n",
    "        val_correlation_spearman.append(val_corr_spearman)\n",
    "\n",
    "        print(f\"Average Training Correlation Pearson: {np.mean(train_correlations_pearson):.3f}\")\n",
    "        print(f\"Average Validation Correlation Pearson: {np.mean(val_correlations_pearson):.3f}\")\n",
    "        print(f\"Average Training Correlation Spearman: {np.mean(train_correlations_spearman):.3f}\")\n",
    "        print(f\"Average Validation Correlation Spearman: {np.mean(val_correlation_spearman):.3f}\")\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation values etc\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def eval_model(model,embeddings_train,embeddings_val,val_labels):#,scaler_preprocess # input: fit model, fit scaler, scaled train features, train embeddings, val embeddings, (val labels.. används inte)\n",
    "\n",
    "        y_train_pred = model.predict(embeddings_train).reshape(-1, 1) # use fit model to predict training labels\n",
    "        y_val_pred = model.predict(embeddings_val).reshape(-1, 1) # use fit model to predict val labels\n",
    "\n",
    "        # Cosine similarity calculations (tror jag får ändra lite här för att just nu är det inte bara embeddings i embeddings arrays)\n",
    "        scaler2=MinMaxScaler() # get scaler for scaling the values for coloring\n",
    "        mean_cos_sim_train_matrix = np.clip(cosine_similarity(embeddings_train), -1.0, 1.0)  # For numerical stability\n",
    "        mean_cos_sim_train = mean_cos_sim_train_matrix.mean(axis=1)\n",
    "        mean_cos_sim_train_c = scaler2.fit_transform(mean_cos_sim_train.reshape(-1, 1)).flatten()\n",
    "\n",
    "        mean_cos_sim_val_matrix = np.clip(cosine_similarity(embeddings_val,embeddings_train), -1.0, 1.0)#np.clip(cosine_similarity(X_val_scaled,X_train_scaled), -1.0, 1.0)\n",
    "        mean_cos_sim_val = mean_cos_sim_val_matrix.mean(axis=1)\n",
    "        mean_cos_sim_val_c = scaler2.transform(mean_cos_sim_val.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Store results\n",
    "        train_predictions=y_train_pred #predicted training labels using fit model\n",
    "        val_predictions=y_val_pred # predicted val labels using fit model\n",
    "        train_mean_cos_sim_matrix=mean_cos_sim_train_matrix # cos sim matrix training embeddings\n",
    "        val_mean_cos_sim_matrix=mean_cos_sim_val_matrix # cos sim matrix val to training embeddings\n",
    "        train_mean_cos_sim_c=mean_cos_sim_train_c # scaled and flattened cos sim values for coloring training\n",
    "        val_mean_cos_sim_c=mean_cos_sim_val_c # scaled and flattened cos sim values for coloring val\n",
    "\n",
    "        return (train_predictions, val_predictions, \n",
    "            train_mean_cos_sim_matrix, val_mean_cos_sim_matrix, \n",
    "            train_mean_cos_sim_c, val_mean_cos_sim_c)\n",
    "# returns: predicted training labels, predicted val labels, train matrix, val matrix, train color vector, val color vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing GridSearchCV...\n",
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n",
      "Best parameters found: {'C': 0.2, 'epsilon': 0.4, 'kernel': 'rbf'}\n",
      "Average Training Correlation Pearson: 0.613\n",
      "Average Validation Correlation Pearson: 0.274\n",
      "Average Training Correlation Spearman: 0.600\n",
      "Average Validation Correlation Spearman: 0.321\n",
      "Average Training Correlation Pearson: 0.611\n",
      "Average Validation Correlation Pearson: 0.250\n",
      "Average Training Correlation Spearman: 0.603\n",
      "Average Validation Correlation Spearman: 0.262\n",
      "Average Training Correlation Pearson: 0.611\n",
      "Average Validation Correlation Pearson: 0.196\n",
      "Average Training Correlation Spearman: 0.602\n",
      "Average Validation Correlation Spearman: 0.208\n",
      "Average Training Correlation Pearson: 0.612\n",
      "Average Validation Correlation Pearson: 0.208\n",
      "Average Training Correlation Spearman: 0.605\n",
      "Average Validation Correlation Spearman: 0.209\n",
      "Average Training Correlation Pearson: 0.610\n",
      "Average Validation Correlation Pearson: 0.216\n",
      "Average Training Correlation Spearman: 0.603\n",
      "Average Validation Correlation Spearman: 0.208\n",
      "Average Training Correlation Pearson: 0.609\n",
      "Average Validation Correlation Pearson: 0.214\n",
      "Average Training Correlation Spearman: 0.603\n",
      "Average Validation Correlation Spearman: 0.210\n",
      "Average Training Correlation Pearson: 0.608\n",
      "Average Validation Correlation Pearson: 0.218\n",
      "Average Training Correlation Spearman: 0.602\n",
      "Average Validation Correlation Spearman: 0.211\n",
      "Average Training Correlation Pearson: 0.608\n",
      "Average Validation Correlation Pearson: 0.226\n",
      "Average Training Correlation Spearman: 0.602\n",
      "Average Validation Correlation Spearman: 0.220\n",
      "Average Training Correlation Pearson: 0.608\n",
      "Average Validation Correlation Pearson: 0.230\n",
      "Average Training Correlation Spearman: 0.603\n",
      "Average Validation Correlation Spearman: 0.224\n",
      "Average Training Correlation Pearson: 0.608\n",
      "Average Validation Correlation Pearson: 0.239\n",
      "Average Training Correlation Spearman: 0.603\n",
      "Average Validation Correlation Spearman: 0.229\n",
      "Validation MSE small error: 0.21752099596716684\n",
      "Validation MSE big error: 3.745423299745528\n"
     ]
    }
   ],
   "source": [
    "# Evaluate \n",
    "#\n",
    "embedding_scaler=StandardScaler()\n",
    "closeness_scaler=StandardScaler()\n",
    "embeddings = np.asarray(combined_df.CLS_embeddings.tolist(),dtype=np.float32) # train embeddings fold i\n",
    "embeddings_scaled=embedding_scaler.fit_transform(embeddings)\n",
    "closeness=combined_df[['distance_to_training']].values\n",
    "closeness_scaled=closeness_scaler.fit_transform(closeness)\n",
    "X=np.hstack([embeddings_scaled,closeness_scaled] )  # Add closeness as a feature\n",
    "\n",
    "labels = np.asarray(combined_df.pred_L1_error.tolist(),dtype=np.float32)#L1['train'][i]  # train errors fold i\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,          # Features or the entire DataFrame\n",
    "    labels,        # Target or None\n",
    "    test_size=0.2, # Proportion of the data to include in the test split\n",
    "    random_state=42, # Random seed for reproducibility\n",
    "    shuffle=True   # Shuffle the data before splitting (default is True)\n",
    ")\n",
    "    # Get the values for this fold\n",
    "model=train_svr_with_cv(X_train=X_train, y_train=y_train,n_splits=10) \n",
    "# split the test into small and big error\n",
    "small_error = y_test < 2\n",
    "big_error = y_test >= 2\n",
    "X_test_small=X_test[small_error]\n",
    "y_test_small=y_test[small_error]\n",
    "X_test_big=X_test[big_error]\n",
    "y_test_big=y_test[big_error]\n",
    "\n",
    "train_pred_small, val_pred_small, mean_cos_sim_train_matrix_small, mean_cos_sim_val_matrix_small, mean_cos_sim_train_c_small, mean_cos_sim_val_c_small=eval_model(model=model,embeddings_train=X_train,embeddings_val=X_test_small,val_labels=y_test_small) #scaler_preprocess=scaler_preprocess,\n",
    "    # Store results\n",
    "train_pred_big, val_pred_big, mean_cos_sim_train_matrix_big, mean_cos_sim_val_matrix_big, mean_cos_sim_train_c_big, mean_cos_sim_val_c_big=eval_model(model=model,embeddings_train=X_train,embeddings_val=X_test_big,val_labels=y_test_big) #scaler_preprocess=scaler_preprocess,\n",
    "    # Store results\n",
    "\n",
    "mse_small=mean_squared_error(y_test_small,val_pred_small)\n",
    "mse_big=mean_squared_error(y_test_big,val_pred_big)\n",
    "print(f\"Validation MSE small error: {mse_small}\")\n",
    "print(f\"Validation MSE big error: {mse_big}\")\n",
    "#print('fold',i,'above 0.2',cnt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation pearson small error: 0.20317460033650697\n",
      "Correlation pearson big error: -0.08598489044674255\n",
      "Correlation spearman small error: 0.19693095458811166\n",
      "Correlation spearman big error: 0.004926108374384235\n"
     ]
    }
   ],
   "source": [
    "corr_small_p=pearsonr(y_test_small,val_pred_small[:,0])[0]\n",
    "corr_big_p=pearsonr(y_test_big,val_pred_big[:,0])[0]\n",
    "corr_small_s=spearmanr(y_test_small,val_pred_small[:,0])[0]\n",
    "corr_big_s=spearmanr(y_test_big,val_pred_big[:,0])[0]\n",
    "print('Correlation pearson small error:',corr_small_p)\n",
    "print('Correlation pearson big error:',corr_big_p)\n",
    "print('Correlation spearman small error:',corr_small_s)\n",
    "print('Correlation spearman big error:',corr_big_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at if those validation chemicals that have large error have any close neighbor in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at how close the closest training embeddings are for every validation with high error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Max similarity per chemical with high error: [0.884709   0.66467077 0.63373476 0.9240259  0.97364414 0.7416723\n",
      " 0.84180105 0.8209464  0.6806984  0.7026757  0.87976414 0.9983338\n",
      " 0.78890955 0.9458796  0.6638415 ]\n",
      "Fold 1:\n",
      "  Max similarity per chemical with high error: [0.9375438  0.93504065 0.68566847 0.68653274 0.6536435  0.9689001\n",
      " 0.73351717 0.65976083 0.7504008  0.88874954]\n",
      "Fold 2:\n",
      "  Max similarity per chemical with high error: [0.9279959  0.6734314  0.97699416 0.6572237  0.5083487  0.953298\n",
      " 0.79454756 0.5474426  0.96011865 0.9429516  0.914539   0.9844668\n",
      " 0.9595736  0.994442   0.47601905 0.42453396 0.96880317 0.9665307\n",
      " 0.7261177  0.9559183  0.9966013 ]\n",
      "Fold 3:\n",
      "  Max similarity per chemical with high error: [0.6389985  0.9204129  0.92116594 0.99182516 0.8236215  0.7449521\n",
      " 0.83042765 0.9638907  0.99557865 0.9718716  0.7279932  0.918617\n",
      " 0.99383193 0.6415573  0.9888354  0.94467026 0.93603414]\n",
      "Fold 4:\n",
      "  Max similarity per chemical with high error: [0.8560637  0.64614254 0.7463119  0.85231745 0.7746514  0.87355673\n",
      " 0.9223889  0.5492747  0.7226336  0.6700999  0.9853165  0.9757544\n",
      " 0.9925188  0.924996   0.94468975]\n",
      "Fold 5:\n",
      "  Max similarity per chemical with high error: [0.68043536 0.58669555 0.9633255  0.8592511  0.56806076 0.6560414\n",
      " 0.9975379  0.68238854 0.5647249  0.97889566 0.6371149  0.96475196\n",
      " 0.6489749  0.9291072  0.9170283 ]\n",
      "Fold 6:\n",
      "  Max similarity per chemical with high error: [0.92905366 0.8051858  0.70583314 0.79059416 0.67501986 0.8291081\n",
      " 0.8796238  0.9163822  0.53178895 0.9364914  0.95980525 0.73511755\n",
      " 0.9023123  0.9802429  0.8746444 ]\n",
      "Fold 7:\n",
      "  Max similarity per chemical with high error: [0.9667884  0.7583127  0.889601   0.9739318  0.9733616  0.93980014\n",
      " 0.82604516 0.9568875  0.7303355  0.95817304 0.93034    0.300223\n",
      " 0.82399905 0.95664525 0.95017326 0.7403996 ]\n",
      "Fold 8:\n",
      "  Max similarity per chemical with high error: [0.8496194  0.8595148  0.9547168  0.7749619  0.5175213  0.9064048\n",
      " 0.94058645 0.9066041  0.9629864 ]\n",
      "Fold 9:\n",
      "  Max similarity per chemical with high error: [0.9502038  0.78454506 0.78454506 0.90470266 0.8209401  0.9101324\n",
      " 0.9274241  0.9442321  0.85518974 0.63284755 0.89903337]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    val = dataframes['val'][i]  # Validation set for fold i\n",
    "    train = dataframes['train'][i]  # Training set for fold i\n",
    "\n",
    "    # Select rows in val with large error\n",
    "    val_filtered = val[val['pred_L1_error'] >= 2]\n",
    "    # Extract embeddings\n",
    "    val_embeddings = np.asarray(val_filtered.CLS_embeddings.tolist(),dtype=np.float32)  \n",
    "    train_embeddings = np.asarray(train.CLS_embeddings.tolist(),dtype=np.float32)  \n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(val_embeddings, train_embeddings)  \n",
    "    # Get the highest similarity for each val_filtered chemical\n",
    "    max_similarities = np.max(similarity_matrix, axis=1)  \n",
    "\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Max similarity per chemical with high error: {max_similarities}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the closest small error values for the big errors validation chemicals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Max similarity per chemical with high error: [0.83342826 0.5791188  0.54923964 0.89632344 0.96700275 0.6698829\n",
      " 0.8006983  0.80460215 0.62537277 0.7878556  0.8749682  0.9448178\n",
      " 0.66178536 0.9044119  0.50094706]\n",
      "  Big error fold count: 15\n",
      "  Closest pred_L1_error values: [0.78441064 0.16061107 1.65824417 0.10869994 0.64233185 0.37375976\n",
      " 0.60755289 0.74742893 0.28620382 1.25163334 0.5509512  0.17918888\n",
      " 0.37142556 1.33654862 0.32381436]\n",
      "  Mean pred_L1_error of closest small errors: 0.6255\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 12\n",
      "\n",
      "Fold 1:\n",
      "  Max similarity per chemical with high error: [0.904271   0.92875385 0.6381161  0.65695083 0.63436437 0.8352196\n",
      " 0.69684166 0.67233187 0.69842625 0.85092866]\n",
      "  Big error fold count: 10\n",
      "  Closest pred_L1_error values: [0.3562168  0.49056661 0.35510807 0.10045689 0.18334256 0.1676905\n",
      " 1.20250578 1.20250578 0.10769362 0.13784475]\n",
      "  Mean pred_L1_error of closest small errors: 0.4304\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 8\n",
      "\n",
      "Fold 2:\n",
      "  Max similarity per chemical with high error: [0.9213373  0.59877574 0.961632   0.6265148  0.42439902 0.9454317\n",
      " 0.77412903 0.43341058 0.8840655  0.9437327  0.8600782  0.624866\n",
      " 0.92619145 0.95988345 0.46699455 0.38608423 0.9144126  0.93328744\n",
      " 0.6940603  0.9374585  0.97358227]\n",
      "  Big error fold count: 21\n",
      "  Closest pred_L1_error values: [0.14893488 0.28657322 0.40739725 0.47045198 0.2181596  0.19984983\n",
      " 1.02559083 0.01926617 1.8241289  1.47766707 0.14893488 0.15582652\n",
      " 0.28518436 0.06268658 0.2243294  0.76875152 0.12560821 0.01452293\n",
      " 0.74234944 0.69917643 0.75418427]\n",
      "  Mean pred_L1_error of closest small errors: 0.4790\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 18\n",
      "\n",
      "Fold 3:\n",
      "  Max similarity per chemical with high error: [0.5945095  0.74367607 0.86544526 0.9646223  0.8247218  0.599994\n",
      " 0.7018949  0.9551559  0.6498444  0.91664386 0.77546173 0.59090924\n",
      " 0.98411417 0.6379338  0.96958035 0.9047455  0.8951113 ]\n",
      "  Big error fold count: 17\n",
      "  Closest pred_L1_error values: [0.39142883 1.15691112 0.23972338 0.52020195 0.65260574 0.31477783\n",
      " 0.49162542 0.42864377 1.64101732 0.56080262 0.50586401 0.06847213\n",
      " 1.14752635 0.04612233 0.29692907 0.303396   0.86947135]\n",
      "  Mean pred_L1_error of closest small errors: 0.5668\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 14\n",
      "\n",
      "Fold 4:\n",
      "  Max similarity per chemical with high error: [0.80891484 0.59382606 0.73407125 0.6938489  0.634932   0.85999286\n",
      " 0.87136775 0.451904   0.6231921  0.5540361  0.95327616 0.9637737\n",
      " 0.9431963  0.92068315 0.780038  ]\n",
      "  Big error fold count: 15\n",
      "  Closest pred_L1_error values: [0.17318838 1.13156207 0.17785691 1.25048694 1.03679419 0.18778222\n",
      " 0.2396528  0.4517455  1.57886332 0.4517455  0.2396528  0.46312557\n",
      " 0.83965949 0.08882006 1.64351645]\n",
      "  Mean pred_L1_error of closest small errors: 0.6636\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 10\n",
      "\n",
      "Fold 5:\n",
      "  Max similarity per chemical with high error: [0.53563064 0.5576124  0.9217396  0.8495013  0.46337262 0.513317\n",
      " 0.9621912  0.54478055 0.5130595  0.98258513 0.59300387 0.82043964\n",
      " 0.6261305  0.88657737 0.93162644]\n",
      "  Big error fold count: 15\n",
      "  Closest pred_L1_error values: [1.00926891 0.69422351 0.51520916 0.26689545 1.13081979 0.45802607\n",
      " 0.13448273 0.4277051  0.162159   0.36692915 0.56216357 0.33149765\n",
      " 0.46737169 0.52671604 1.00926891]\n",
      "  Mean pred_L1_error of closest small errors: 0.5375\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 12\n",
      "\n",
      "Fold 6:\n",
      "  Max similarity per chemical with high error: [0.84205014 0.50633746 0.9186114  0.7101531  0.62288535 0.80582654\n",
      " 0.823408   0.8859328  0.47678885 0.93791646 0.9394927  0.7048676\n",
      " 0.690206   0.97438264 0.78705335]\n",
      "  Big error fold count: 15\n",
      "  Closest pred_L1_error values: [1.52452877 1.12097541 0.10404898 0.3412277  1.43971712 0.51112881\n",
      " 0.62002613 0.54219545 0.03247433 0.14679784 1.0598397  0.26935283\n",
      " 0.47323472 0.20543512 0.55300593]\n",
      "  Mean pred_L1_error of closest small errors: 0.5963\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 11\n",
      "\n",
      "Fold 7:\n",
      "  Max similarity per chemical with high error: [0.9338139  0.7106722  0.8686898  0.95024204 0.8463031  0.88329697\n",
      " 0.64678323 0.69631374 0.49551284 0.9429252  0.6484077  0.3511631\n",
      " 0.76612455 0.95677686 0.6924309  0.692526  ]\n",
      "  Big error fold count: 16\n",
      "  Closest pred_L1_error values: [0.64686399 1.37423344 0.18571973 1.14966263 0.82968475 0.76919583\n",
      " 0.66048327 0.52673546 0.42846166 0.09093094 0.20120255 1.16923144\n",
      " 0.5534489  0.64686399 0.50629436 0.94789903]\n",
      "  Mean pred_L1_error of closest small errors: 0.6679\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 13\n",
      "\n",
      "Fold 8:\n",
      "  Max similarity per chemical with high error: [0.83787477 0.8296883  0.82742655 0.73543435 0.4432702  0.9040981\n",
      " 0.9156307  0.8894761  0.949329  ]\n",
      "  Big error fold count: 9\n",
      "  Closest pred_L1_error values: [0.09462147 1.39314849 0.6906326  0.58816499 0.84869914 0.35336303\n",
      " 0.56836683 0.09042528 0.35630846]\n",
      "  Mean pred_L1_error of closest small errors: 0.5537\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 8\n",
      "\n",
      "Fold 9:\n",
      "  Max similarity per chemical with high error: [0.94845855 0.595386   0.595386   0.4459154  0.9281763  0.78601325\n",
      " 0.91561735 0.9201945  0.8519896  0.5924947  0.7983996 ]\n",
      "  Big error fold count: 11\n",
      "  Closest pred_L1_error values: [0.18034394 0.43183059 0.43183059 0.71835912 0.62441487 1.0153619\n",
      " 0.12830421 0.59601449 1.49311163 0.32661347 0.1285064 ]\n",
      "  Mean pred_L1_error of closest small errors: 0.5522\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    val = dataframes['val'][i]  \n",
    "\n",
    "    # Separate val into high and low errors\n",
    "    val_filtered = val[val['pred_L1_error'] >= 2]  # High-error \n",
    "    val_remaining = val[val['pred_L1_error'] < 2]  # Small error\n",
    "\n",
    "    # Extract embeddings \n",
    "    val_filtered_embeddings = np.asarray(val_filtered.CLS_embeddings.tolist(),dtype=np.float32)\n",
    "    val_remaining_embeddings = np.asarray(val_remaining.CLS_embeddings.tolist(),dtype=np.float32)\n",
    "\n",
    "    # Compute cosine similarity between small and big error validation embeddings\n",
    "    similarity_matrix = cosine_similarity(val_filtered_embeddings, val_remaining_embeddings)  \n",
    "\n",
    "    # Find the index of the closest small error for each big error\n",
    "    closest_indices = np.argmax(similarity_matrix, axis=1) \n",
    "\n",
    "    # Retrieve the closest small chemicals pred_L1_error values\n",
    "    closest_errors = val_remaining.iloc[closest_indices]['pred_L1_error'].values  \n",
    "    max_similarities = np.max(similarity_matrix, axis=1)  \n",
    "\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Max similarity per chemical with high error: {max_similarities}\")\n",
    "    print(f\"  Big error fold count: {len(val_filtered)}\")\n",
    "    print(f\"  Closest pred_L1_error values: {closest_errors}\")\n",
    "    print(f\"  Mean pred_L1_error of closest small errors: {np.mean(closest_errors):.4f}\")\n",
    "    print(f\"  Number of closest small error embeddings with pred_L1_error < 1: {np.sum(closest_errors < 1)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Max similarity per chemical with high error: [0.7203343  0.35051635 0.5510047  0.6439332  0.7647406  0.57588166\n",
      " 0.7647406  0.6439332  0.6228596  0.49351174 0.40805417 0.5563406\n",
      " 0.6286299  0.3919394  0.5510047 ]\n",
      "  Big error fold count: 15\n",
      "  Closest pred_L1_error values: [2.47630418 2.69463174 2.10491085 2.40130989 2.47630418 2.40130989\n",
      " 2.42181412 2.43542742 2.42181412 2.42181412 2.40130989 2.86153371\n",
      " 2.86153371 2.43542742 2.8289327 ]\n",
      "  Mean pred_L1_error of closest small errors: 2.5096\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 0\n",
      "\n",
      "Fold 1:\n",
      "  Max similarity per chemical with high error: [0.7707505  0.7796489  0.3672768  0.42884138 0.45590448 0.6541548\n",
      " 0.903974   0.903974   0.47958073 0.7796489 ]\n",
      "  Big error fold count: 10\n",
      "  Closest pred_L1_error values: [2.21194699 3.89077067 2.91243619 2.70461997 2.15855413 2.44252166\n",
      " 2.70461997 2.44252166 2.08709535 2.21194699]\n",
      "  Mean pred_L1_error of closest small errors: 2.5767\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 0\n",
      "\n",
      "Fold 2:\n",
      "  Max similarity per chemical with high error: [0.81612796 0.39854032 0.90543723 0.39854032 0.38930425 0.90543723\n",
      " 0.6477864  0.3528831  0.69056606 0.89994013 0.81612796 0.4125365\n",
      " 0.88080585 0.8734509  0.32202876 0.38930425 0.8734509  0.8879458\n",
      " 0.59069574 0.85061634 0.89994013]\n",
      "  Big error fold count: 21\n",
      "  Closest pred_L1_error values: [3.01967013 2.21340738 2.65053646 2.22452118 2.12148672 3.19626638\n",
      " 2.98670255 2.03716503 3.01967013 2.65043065 2.03716503 2.4501852\n",
      " 2.65043065 3.03715925 2.12148672 3.22634337 2.25723899 3.19626638\n",
      " 3.19626638 2.04640074 2.98670255]\n",
      "  Mean pred_L1_error of closest small errors: 2.6345\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 0\n",
      "\n",
      "Fold 3:\n",
      "  Max similarity per chemical with high error: [0.35301447 0.40205085 0.40205085 0.58079976 0.7002518  0.53198314\n",
      " 0.57161164 0.7799877  0.77965724 0.7799877  0.36306155 0.77965724\n",
      " 0.7013499  0.5557121  0.85476136 0.85476136 0.7607981 ]\n",
      "  Big error fold count: 17\n",
      "  Closest pred_L1_error values: [2.16161565 2.89233168 2.05598894 2.37953822 3.36198832 2.36474098\n",
      " 2.37953822 2.11977618 2.73427129 2.54312052 2.89233168 2.37953822\n",
      " 3.95554728 2.1756328  3.95554728 2.08001505 2.11977618]\n",
      "  Mean pred_L1_error of closest small errors: 2.6207\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 0\n",
      "\n",
      "Fold 4:\n",
      "  Max similarity per chemical with high error: [0.7407002  0.32683903 0.48590994 0.5570245  0.5570245  0.71116024\n",
      " 0.8578952  0.370669   0.5132827  0.39616662 0.94029254 0.94029254\n",
      " 0.7407002  0.71116024 0.5839385 ]\n",
      "  Big error fold count: 15\n",
      "  Closest pred_L1_error values: [2.59842959 2.05236058 2.67522392 2.47012601 2.232584   2.77334194\n",
      " 4.01086704 2.232584   2.59842959 2.39334674 2.30380401 4.01086704\n",
      " 2.39334674 2.10953649 2.67522392]\n",
      "  Mean pred_L1_error of closest small errors: 2.6353\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 0\n",
      "\n",
      "Fold 5:\n",
      "  Max similarity per chemical with high error: [0.53472775 0.42759478 0.8307168  0.69760406 0.34547955 0.4616689\n",
      " 0.9551159  0.4435255  0.3883924  0.9551159  0.4435255  0.57663095\n",
      " 0.44860798 0.7088566  0.57663095]\n",
      "  Big error fold count: 15\n",
      "  Closest pred_L1_error values: [2.27234249 2.48500369 2.14787976 2.14787976 3.28962048 2.27234249\n",
      " 2.48500369 2.2841533  2.48500369 2.14787976 2.08996124 2.27234249\n",
      " 3.28962048 2.48500369 3.34753015]\n",
      "  Mean pred_L1_error of closest small errors: 2.5001\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 0\n",
      "\n",
      "Fold 6:\n",
      "  Max similarity per chemical with high error: [0.78243697 0.38991845 0.3864455  0.565062   0.37874085 0.63022363\n",
      " 0.7810712  0.81536597 0.26607406 0.81536597 0.78243697 0.5982488\n",
      " 0.5982488  0.81415457 0.5596268 ]\n",
      "  Big error fold count: 15\n",
      "  Closest pred_L1_error values: [4.75567974 2.06159409 3.85132727 2.3472771  2.07556849 2.06159409\n",
      " 2.41969727 2.2687369  2.32630858 2.85225861 3.85132727 2.11111376\n",
      " 2.04358901 2.85225861 2.04358901]\n",
      "  Mean pred_L1_error of closest small errors: 2.6615\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 0\n",
      "\n",
      "Fold 7:\n",
      "  Max similarity per chemical with high error: [0.93907875 0.28189823 0.7589075  0.2149975  0.6993983  0.5722078\n",
      " 0.45404163 0.47511587 0.5010936  0.7946646  0.4932199  0.30030742\n",
      " 0.49092603 0.93907875 0.5010936  0.5722078 ]\n",
      "  Big error fold count: 16\n",
      "  Closest pred_L1_error values: [2.67091066 2.4567933  3.24983455 2.4567933  2.00843311 2.4567933\n",
      " 2.2214684  2.3239141  2.2214684  2.33175391 2.31823712 2.0430645\n",
      " 2.3239141  2.33175391 2.29241108 2.3239141 ]\n",
      "  Mean pred_L1_error of closest small errors: 2.3770\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 0\n",
      "\n",
      "Fold 8:\n",
      "  Max similarity per chemical with high error: [0.5551803  0.7215698  0.35042435 0.5551803  0.32771924 0.4872292\n",
      " 0.7764996  0.37325388 0.7764996 ]\n",
      "  Big error fold count: 9\n",
      "  Closest pred_L1_error values: [2.33595182 2.27073702 2.33595182 2.21457446 2.07149427 3.06096595\n",
      " 2.27073702 2.21457446 3.06096595]\n",
      "  Mean pred_L1_error of closest small errors: 2.4262\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 0\n",
      "\n",
      "Fold 9:\n",
      "  Max similarity per chemical with high error: [0.76343983 1.0000002  1.0000002  0.3149138  0.3679328  0.57907593\n",
      " 0.8673775  0.7989821  0.8673775  0.4699468  0.5407691 ]\n",
      "  Big error fold count: 11\n",
      "  Closest pred_L1_error values: [3.20002189 3.35722632 3.28726789 3.28726789 2.42371416 2.44387912\n",
      " 2.65789764 2.44387912 2.44387912 2.20488419 2.30499572]\n",
      "  Mean pred_L1_error of closest small errors: 2.7323\n",
      "  Number of closest small error embeddings with pred_L1_error < 1: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    val = dataframes['val'][i]  \n",
    "\n",
    "    # Separate val into high and low errors\n",
    "    val_filtered = val[val['pred_L1_error'] >= 2]  # High-error \n",
    "\n",
    "    # Extract embeddings \n",
    "    val_filtered_embeddings = np.asarray(val_filtered.CLS_embeddings.tolist(),dtype=np.float32)\n",
    "\n",
    "    # Compute cosine similarity between small and big error validation embeddings\n",
    "    similarity_matrix = cosine_similarity(val_filtered_embeddings)  \n",
    "    np.fill_diagonal(similarity_matrix, -1) # to exclude similarity to oneself\n",
    "\n",
    "    # Find the index of the closest small error for each big error\n",
    "    closest_indices = np.argmax(similarity_matrix, axis=1) \n",
    "\n",
    "    # Retrieve the closest small chemicals pred_L1_error values\n",
    "    closest_errors = val_filtered.iloc[closest_indices]['pred_L1_error'].values  \n",
    "    max_similarities = np.max(similarity_matrix, axis=1)  \n",
    "\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Max similarity per chemical with high error: {max_similarities}\")\n",
    "    print(f\"  Big error fold count: {len(val_filtered)}\")\n",
    "    print(f\"  Closest pred_L1_error values: {closest_errors}\")\n",
    "    print(f\"  Mean pred_L1_error of closest big errors: {np.mean(closest_errors):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trident_plot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
